<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">

<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">


<title>Wei Ping's Homepage</title>
<style type="text/css">
  img.rounded-img {
    border: 1px solid #eeeeee;
    border-radius: 5px ;
    -moz-border-radius: 5px ;
    -webkit-border-radius: 5px ;
  }
<!--
.STYLE1 {font-family: "Times New Roman", Times, serif}
.STYLE2 {font-family: "Times New Roman", Times, serif;
     line-height: 1.4;
}
.STYLE3 {font-family: Arial, Helvetica, sans-serif}
a:link {
    color: mediumblue;
}
a:visited {
    color: mediumblue;
}
a:hover {
    color: blue;
}
a:active {
    color: red
}
-->
</style>
</head>


<body>
<table border="0" cellspacing="20">
  <tbody><tr>
    <td><img class="rounded-img" src="profile.jpg" style="height:310px"></td>
    <td width="30"></td>
    <td valign="top">
      <h1><font size="6">Wei Ping</font></h1>
      <font size="4"> Principal Research Scientist<br></font>
      <font size="4"> NVIDIA<br></font>
      <font size="4"> Santa Clara, CA<br></font>
      <br>
      <font size="4"> Email: wping at nvidia dot com<br></font>
      <br><br>
      <font size="4"> <a href="https://scholar.google.com/citations?user=6gKEYRgAAAAJ&hl=en" target="_blank">Google Scholar</a> &nbsp; </font>     
      <font size="4"> <a href="https://arxiv.org/find/cs/1/au:+Ping_W/0/1/0/all/0/1" target="_blank">arXiv</a> &nbsp; </font>  
      <br>
      <font size="4"> <a href="https://www.linkedin.com/in/wei-ping/" target="_blank">LinkedIn</a> &nbsp; </font>
      <font size="4"> <a href="https://twitter.com/_weiping" target="_blank">Twitter</a> &nbsp; <br></font>
    </td>
  </tr></tbody>
</table>


<p></p>
<h2 class="STYLE1">About me</h2>
<table border="0">
  <tr>
    <p class="STYLE2">
    <!--<br />-->
    <font size="4"> 
    I am a principal research scientist at NVIDIA Applied Deep Learning Research team, working on large language models and generative models.
    Prior to this, I was the text-to-speech team lead at Baidu Silicon Valley AI Lab (founded by Andrew Ng).
    Before that, I obtained my PhD in machine learning from University of California, Irvine in 2016.
    I am passionate about building the state-of-the-art generative models for text, audio and multi-modal data.
    </font>
    </p>
    <td>&nbsp;</td>
  </tr>
</table>
<hr/>

<h2 class="STYLE1">Selected Publications</h2>
<strong>*</strong> My student intern
<table border="0">
  <tr>
  <font size="4">
        <p class="STYLE2">
        <a href="https://arxiv.org/abs/2310.03025" target="_blank">Retrieval meets Long Context Large Language Models</a>.&nbsp;<em>preprint</em> 2023.
        <br>Peng Xu, <strong>Wei Ping</strong>, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, Bryan Catanzaro.
        </p>

        <p class="STYLE2">
        <a href="https://arxiv.org/abs/2310.03025" target="_blank">InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining</a>.&nbsp;<em>preprint</em> 2023.
        <br>Boxin Wang*, <strong>Wei Ping</strong>, Lawrence McAfee, Peng Xu, Bo Li, Mohammad Shoeybi, Bryan Catanzaro.
        </p>

        <p class="STYLE2">
        <a href="https://arxiv.org/abs/2304.06762" target="_blank">Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study</a>.&nbsp;[<a href="https://github.com/NVIDIA/Megatron-LM#retro" target="_blank">code</a>]&nbsp;&nbsp;<em>EMNLP</em> 2023.
        <br>Boxin Wang*, <strong>Wei Ping</strong>, Peng Xu, Lawrence McAfee, Zihan Liu, Mohammad Shoeybi et al.
        </p>

        <p class="STYLE2">
        <a href="https://arxiv.org/abs/2302.04858" target="_blank">Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Captioning</a>.&nbsp;<em>Findings of EMNLP</em> 2023.
        <br>Zhuolin Yang*, <strong>Wei Ping</strong>, Zihan Liu, Vijay Korthikanti, Weili Nie et al.
        </p>

        <p class="STYLE2">
        <a href="https://arxiv.org/abs/2206.04658" target="_blank">BigVGAN: A Universal Neural Vocoder with Large-Scale Training</a>.&nbsp;[<a href="https://bigvgan-demo.github.io/" target="_blank">demo</a>]&nbsp;[<a href="https://github.com/NVIDIA/BigVGAN" target="_blank">code</a>]&nbsp;&nbsp;<em>ICLR</em> 2023.
        <br>Sang-gil Lee*, <strong>Wei Ping</strong>, Boris Ginsburg, Bryan Catanzaro, Sungroh Yoon.
        </p>

        <p class="STYLE2">
        <a href="https://arxiv.org/abs/2206.04624" target="_blank">Factuality Enhanced Language Models for Open-Ended Text Generation</a>.&nbsp;[<a href="https://github.com/nayeon7lee/FactualityPrompt" target="_blank">code</a>]&nbsp;&nbsp;<em>NeurIPS</em> 2022.
        <br>Nayeon Lee*, <strong>Wei Ping</strong>, Peng Xu, Mostofa Patwary, Pascale Fung, Mohammad Shoeybi, Bryan Catanzaro.
        </p>

        <p class="STYLE2">
        <a href="https://arxiv.org/abs/2202.04173" target="_blank">Exploring the Limits of Domain-Adaptive Training for Detoxifying Large-Scale Language Models</a>.&nbsp;[<a href="https://github.com/NVIDIA/Megatron-LM/tree/main/examples/detxoify_lm" target="_blank">code</a>]&nbsp;&nbsp;<em>NeurIPS</em> 2022.
        <br>Boxin Wang*, <strong>Wei Ping</strong>, Chaowei Xiao, Peng Xu, Mostofa Patwary, Mohammad Shoeybi, Bo Li, Anima Anandkumar, Bryan Catanzaro.
        </p>

        <p class="STYLE2">
        <a href="https://arxiv.org/pdf/2107.02192.pdf" target="_blank">Long-Short Transformer: Efficient transformers for language and vision</a>.&nbsp;[<a href="https://github.com/NVIDIA/transformer-ls" target="_blank">code</a>]&nbsp;&nbsp;<em>NeurIPS</em> 2021.
        <br>Chen Zhu*, <strong>Wei Ping</strong>, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein, Anima Anandkumar, Bryan Catanzaro.
        </p>

        <p class="STYLE2">
        <a href="https://arxiv.org/pdf/2009.09761.pdf" target="_blank">DiffWave: A versatile diffusion model for audio synthesis</a>.&nbsp;[<a href="https://diffwave-demo.github.io/" target="_blank">project</a>]&nbsp;&nbsp;<em>ICLR</em> 2021 (Oral).
        <br>Zhifeng Kong*, <strong>Wei Ping</strong>, Jiaji Huang, Kexin Zhao, Bryan Catanzaro.
        </p>
    
        <p class="STYLE2">
        <a href="https://arxiv.org/pdf/1912.01219.pdf" target="_blank">WaveFlow: A compact flow-based model for raw audio</a>.&nbsp;[<a href="https://waveflow-demo.github.io/" target="_blank">project</a>]&nbsp;&nbsp;<em>ICML</em> 2020.
        <br><strong>Wei Ping</strong>, Kainan Peng, Kexin Zhao, Zhao Song.
        </p>

        <p class="STYLE2">
        <a href="https://arxiv.org/pdf/1905.08459.pdf" target="_blank">Non-autoregressive neural text-to-speech</a>.&nbsp;[<a href="https://parallel-neural-tts-demo.github.io/" target="_blank">demo</a>]&nbsp;&nbsp;<em>ICML</em> 2020.
        <br>Kainan Peng, <strong>Wei Ping</strong>, Zhao Song, Kexin Zhao.
        </p>

        <p class="STYLE2">
        <a href="https://arxiv.org/pdf/1807.07281.pdf" target="_blank">ClariNet: Parallel wave generation in end-to-end text-to-speech</a>.&nbsp;[<a href="https://clarinet-demo.github.io/" target="_blank">demo</a>]&nbsp;&nbsp;<em>ICLR</em> 2019.
        <br><strong>Wei Ping</strong>, Kainan Peng, Jitong Chen.
        </p>

        <p class="STYLE2">
        <a href="https://arxiv.org/pdf/1710.07654.pdf" target="_blank">Deep Voice 3: Scaling text-to-speech with convolutional sequence learning</a>.&nbsp;</a>[<a href="http://research.baidu.com/Blog/index-view?id=91" target="_blank">demo</a>][<a href="https://multi-speaker-clarinet-demo.github.io/" target="_blank">more samples</a>]&nbsp;&nbsp;<em>ICLR</em> 2018.
        <br><strong>Wei Ping</strong>, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang, Jonathan Raiman, John Miller.
        </p>

        <p class="STYLE2">
        <a href="https://arxiv.org/pdf/1802.06006" target="_blank">Neural voice cloning with a few samples</a>.&nbsp;<a href="https://audiodemos.github.io/" target="_blank">[demo]</a>&nbsp;&nbsp;<em>NeurIPS</em> 2018. 
        <br>Sercan O. Arik, Jitong Chen, Kainan Peng, <strong>Wei Ping</strong>, Yanqi Zhou.
        </p>

        <p class="STYLE2">
        <a href="https://arxiv.org/pdf/1705.08947.pdf" target="_blank">Deep Voice 2: Multi-speaker neural text-to-speech</a>.&nbsp;<a href="http://research.baidu.com/Blog/index-view?id=97" target="_blank">[demo]</a>&nbsp;&nbsp;<em>NIPS</em> 2017.
        <br>Sercan O. Arik, Gregory Diamos, Andrew Gibiansky, John Miller, Kainan Peng, <strong>Wei Ping</strong>, Jonathan Raiman, Yanqi Zhou.
        </p>

        <p class="STYLE2"> 
        <a href="http://papers.nips.cc/paper/6342-learning-infinite-rbms-with-frank-wolfe.pdf" target="_blank">Learning infinite RBMs with Frank-Wolfe</a>.&nbsp;<em>NIPS</em> 2016.
        <br><strong>Wei Ping</strong>, Qiang Liu, Alexander Ihler.
        </p>

        <p class="STYLE2"> 
        <a href="https://arxiv.org/pdf/1511.02619.pdf" target="_blank">Decomposition bounds for marginal MAP</a>.&nbsp;&nbsp;</a><em>NIPS</em> 2015.
        <br><strong>Wei Ping</strong>, Qiang Liu, Alexander Ihler.
        </p>

        <p class="STYLE2"> 
        <a href="http://proceedings.mlr.press/v32/ping14.pdf" target="_blank">Marginal structured SVM with hidden variables</a>.&nbsp;&nbsp;</a><em>ICML</em> 2014.
        <br><strong>Wei Ping</strong>, Qiang Liu, Alexander Ihler.
        </p>
  </font>
  </tr>
</table>
<hr />


</body>
</html>

