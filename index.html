<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">

<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">


<title>Wei Ping 平伟</title>
<style type="text/css">
  img.rounded-img {
    border: 1px solid #eeeeee;
    border-radius: 5px ;
    -moz-border-radius: 5px ;
    -webkit-border-radius: 5px ;
  }
<!--
.STYLE1 {font-family: "Times New Roman", Times, serif}
.STYLE2 {font-family: "Times New Roman", Times, serif;
     line-height: 1.4;
}
.STYLE3 {font-family: Arial, Helvetica, sans-serif}
a:link {
    color: mediumblue;
}
a:visited {
    color: mediumblue;
}
a:hover {
    color: blue;
}
a:active {
    color: red
}
-->
</style>
</head>


<body>
<table border="0" cellspacing="20">
  <tbody><tr>
    <td><img class="rounded-img" src="profile.jpg" style="height:310px"></td>
    <td width="30"></td>
    <td valign="top">
      <h1><font size="6">Wei Ping 平伟</font></h1>
      <font size="4"> Distinguished Research Scientist<br></font>
      <font size="4"> NVIDIA<br></font>
      <font size="4"> Santa Clara, CA<br></font>
      <br>
      <font size="4"> Email: wping at nvidia dot com<br></font>
      <br>
      <font size="4"> For general academic work: weiping.thu at gmail dot com<br></font>
      <br><br>
      <font size="4"> <a href="https://scholar.google.com/citations?user=6gKEYRgAAAAJ&hl=en" target="_blank">Google Scholar</a> &nbsp; </font>     
      <font size="4"> <a href="https://arxiv.org/find/cs/1/au:+Ping_W/0/1/0/all/0/1" target="_blank">arXiv</a> &nbsp; </font>  
      <br>
      <font size="4"> <a href="https://www.linkedin.com/in/wei-ping/" target="_blank">LinkedIn</a> &nbsp; </font>
      <font size="4"> <a href="https://twitter.com/_weiping" target="_blank">Twitter</a> &nbsp; <br></font>
    </td>
  </tr></tbody>
</table>


<p></p>
<h2 class="STYLE1">About me</h2>
<table border="0">
  <tr>
    <p class="STYLE2">
    <!--<br />-->
    <font size="4"> 
      I am a Distinguished Research Scientist at NVIDIA, working on large language models with a focus on post-training and multimodality.
      Before that, I worked on generative models and speech synthesis at Baidu's Silicon Valley AI Lab, founded by Andrew Ng.
      During my Ph.D at UC Irvine, I researched probabilistic graphical models and variational inference.
    </font>
    </p>
    <td>&nbsp;</td>
  </tr>
</table>
<hr/>

<h2 class="STYLE1">Selected Publications</h2>
Find the full list on <a href="https://scholar.google.com/citations?hl=en&user=6gKEYRgAAAAJ&view_op=list_works&sortby=pubdate" target="_blank">Google Scholar</a>.
<br>
<br>
<strong>*</strong> Equal contribution. &nbsp;
<strong><sup>+</sup></strong> My student intern.

<table border="0">
  <tr>
  <font size="4">
        <p class="STYLE2">
        <a href="https://arxiv.org/abs/2506.13284" target="_blank">AceReason-Nemotron 1.1: Advancing math and code reasoning through through SFT and RL synergy</a>.&nbsp;&nbsp;[<a href="https://huggingface.co/nvidia/AceReason-Nemotron-1.1-7B" target="_blank">checkpoint and data</a>]&nbsp; 2025.
        <br>Zihan Liu, Zhuolin Yang, Yang Chen, Chankyu Lee, Mohammad Shoeybi, Bryan Catanzaro, <strong>Wei Ping</strong>.
        </p>
    
        <p class="STYLE2">
        <a href="https://arxiv.org/abs/2505.16400" target="_blank">AceReason-Nemotron: Advancing math and code reasoning through reinforcement learning</a>.&nbsp;&nbsp;[<a href="https://huggingface.co/collections/nvidia/acereason-682f4e1261dc22f697fd1485" target="_blank">checkpoints and data</a>]&nbsp; 2025.
        <br>Yang Chen*, Zhuolin Yang*, Zihan Liu, Chankyu Lee, Peng Xu, Mohammad Shoeybi, Bryan Catanzaro, <strong>Wei Ping</strong>.
        </p>

        <p class="STYLE2">
        <a href="https://arxiv.org/abs/2504.03624" target="_blank">Nemotron-H: A family of accurate and efficient hybrid mamba-transformer models</a>.&nbsp;&nbsp;[<a href="https://research.nvidia.com/labs/adlr/nemotronh/" target="_blank">blog</a>]&nbsp;[<a href="https://huggingface.co/collections/nvidia/nemotron-h-67fd3d7ca332cdf1eb5a24bb" target="_blank">checkpoints</a>]&nbsp; 2025.
        <br>NVIDIA: <strong>Wei Ping</strong> (core contributor).
        </p>
    
        <p class="STYLE2">
        <a href="https://arxiv.org/abs/2503.15558" target="_blank">Cosmos-Reason1: From physical common sense to embodied reasoning</a>.&nbsp;&nbsp;[<a href="https://research.nvidia.com/labs/dir/cosmos-reason1/" target="_blank">project</a>]&nbsp;[<a href="https://nvidianews.nvidia.com/news/nvidia-announces-major-release-of-cosmos-world-foundation-models-and-physical-ai-data-tools" target="_blank">blog</a>]&nbsp; 2025.
        <br>NVIDIA: <strong>Wei Ping</strong> (core contributor).
        </p>
    
        <p class="STYLE2">
        <a href="https://arxiv.org/abs/2412.15084" target="_blank">AceMath: Advancing frontier math reasoning with post-training and reward modeling</a>.&nbsp;&nbsp;[<a href="https://research.nvidia.com/labs/adlr/acemath/" target="_blank">project</a>]&nbsp;[<a href="https://huggingface.co/collections/nvidia/acemath-678917d12f09885479d549fe" target="_blank">checkpoints and training data</a>]&nbsp; <em>ACL</em> 2025.
        <br>Zihan Liu*, Yang Chen*, Mohammad Shoeybi, Bryan Catanzaro, <strong>Wei Ping</strong>.
        </p>

        <p class="STYLE2">
        <a href="https://arxiv.org/abs/2409.11402" target="_blank">NVLM: Open frontier-class multimodal LLMs</a>.&nbsp;&nbsp;[<a href="https://nvlm-project.github.io/" target="_blank">project</a>]&nbsp;[<a href="https://huggingface.co/collections/nvidia/nvlm-10-66e9f407c764a0ee6e37b7f4" target="_blank">checkpoints</a>]&nbsp;[<a href="https://github.com/NVIDIA/Megatron-LM/tree/NVLM-1.0/examples/multimodal/nvlm" target="_blank">training code</a>]&nbsp; Technical Report. 2024.
        <br>Wenliang Dai*, Nayeon Lee*, Boxin Wang*, Zhuolin Yang*, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, <strong>Wei Ping*</strong>.
        </p>
    
        <p class="STYLE2">
        <a href="https://arxiv.org/abs/2405.17428" target="_blank">NV-Embed: Improved techniques for training LLMs as generalist embedding models</a>.&nbsp;&nbsp;[<a href="https://huggingface.co/collections/nvidia/nv-embed-66588aba24f3d17e4bdd3c3a" target="_blank">checkpoints</a>]&nbsp;&nbsp;<em>ICLR</em> 2025. 
        <br>[NV-Embed-v1 ranked No. 1 on the <a href="https://huggingface.co/spaces/mteb/leaderboard_legacy" target="_blank">MTEB</a> Benchmark as of May 24, 2024. NV-Embed-v2 reclaimed No.1 as of Aug 30, 2024.]
        <br>[We built multimodal retriver <a href="https://arxiv.org/abs/2411.02571" target="_blank">MM-Embed</a> (<em>ICLR</em> 2025) based on NV-Embed-v1]
        <br>Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, <strong>Wei Ping</strong>.
        </p>
    
        <p class="STYLE2">
        <a href="https://arxiv.org/abs/2407.14482" target="_blank">ChatQA 2: Bridging the gap to proprietary LLMs in long context and RAG capabilities</a>.&nbsp;&nbsp;[<a href="https://chatqa2-project.github.io/" target="_blank">project</a>]&nbsp;[<a href="https://huggingface.co/collections/nvidia/llama3-chatqa-2-66ac05b20359211921e14806" target="_blank">checkpoints</a>]&nbsp;<em>ICLR</em> 2025.
        <br>Peng Xu, <strong>Wei Ping</strong>, Xianchao Wu, Zihan Liu, Mohammad Shoeybi, Bryan Catanzaro.
        </p>

        <p class="STYLE2">
        <a href="https://arxiv.org/abs/2401.10225" target="_blank">ChatQA: Surpassing GPT-4 on conversational QA and RAG</a>.&nbsp;&nbsp;[<a href="https://chatqa-project.github.io/" target="_blank">project</a>]&nbsp;[<a href="https://huggingface.co/collections/nvidia/llama3-chatqa-15-662ebbf6acc85f5c444029a8" target="_blank">checkpoints</a>]&nbsp;<em>NeurIPS</em> 2024.
        <br>Zihan Liu, <strong>Wei Ping</strong>, Rajarshi Roy, Peng Xu, Chankyu Lee, Mohammad Shoeybi, Bryan Catanzaro.
        </p>

        <p class="STYLE2">
        <a href="https://arxiv.org/abs/2407.02485" target="_blank">RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs</a>.&nbsp;<em>NeurIPS</em> 2024. 
        <br>Yue Yu<strong><sup>+</sup></strong>, <strong>Wei Ping</strong>, Zihan Liu, Boxin Wang, Jiaxuan You, Chao Zhang, Mohammad Shoeybi, Bryan Catanzaro.
        </p>

        <p class="STYLE2">
        <a href="https://arxiv.org/abs/2310.07713" target="_blank">InstructRetro: Instruction tuning post retrieval-augmented pretraining</a>.&nbsp;[<a href="https://github.com/NVIDIA/Megatron-LM/blob/main/tools/retro/README.md" target="_blank">code</a>]&nbsp;[<a href="https://huggingface.co/collections/nvidia/instructretro-65837ea76b60651e01faec8d" target="_blank">model</a>]&nbsp;&nbsp;<em>ICML</em> 2024.
        <br>Boxin Wang<strong><sup>+</sup></strong>, <strong>Wei Ping</strong>, Lawrence McAfee, Peng Xu, Bo Li, Mohammad Shoeybi, Bryan Catanzaro.
        </p>
           
        <p class="STYLE2">
        <a href="https://arxiv.org/abs/2402.01831" target="_blank">Audio Flamingo: A Novel Audio Language Model with Few-Shot Learning and Dialogue Abilities</a>.&nbsp;&nbsp;[<a href="https://github.com/NVIDIA/audio-flamingo" target="_blank">code</a>]&nbsp;&nbsp;<em>ICML</em> 2024.
        <br>Zhifeng Kong, Arushi Goel, Rohan Badlani, <strong>Wei Ping</strong>, Rafael Valle, Bryan Catanzaro.
        </p>

        <p class="STYLE2">
        <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Lin_VILA_On_Pre-training_for_Visual_Language_Models_CVPR_2024_paper.pdf" target="_blank">VILA: On Pre-training for Visual Language Models</a>.&nbsp;<em>CVPR</em> 2024.
        <br>Ji Lin, Hongxu Yin, <strong>Wei Ping</strong>, Pavlo Molchanov, Mohammad Shoeybi, Song Han.
        </p> 
   

        <p class="STYLE2">
        <a href="https://arxiv.org/abs/2310.03025" target="_blank">Retrieval meets Long Context Large Language Models</a>.&nbsp;<em>ICLR</em> 2024.
        <br>Peng Xu, <strong>Wei Ping</strong>, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, Bryan Catanzaro.
        </p>
    
        <p class="STYLE2">
        <a href="https://arxiv.org/abs/2304.06762" target="_blank">Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study</a>.&nbsp;[<a href="https://github.com/NVIDIA/Megatron-LM/blob/main/tools/retro/README.md" target="_blank">code</a>]&nbsp;&nbsp;<em>EMNLP</em> 2023.
        <br>Boxin Wang<strong><sup>+</sup></strong>, <strong>Wei Ping</strong>, Peng Xu, Lawrence McAfee, Zihan Liu, Mohammad Shoeybi et al.
        </p>

<!--         <p class="STYLE2">
        <a href="https://arxiv.org/abs/2302.04858" target="_blank">Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Captioning</a>.&nbsp;<em>Findings of EMNLP</em> 2023.
        <br>Zhuolin Yang<strong><sup>+</sup></strong>, <strong>Wei Ping</strong>, Zihan Liu, Vijay Korthikanti, Weili Nie et al.
        </p> -->

        <p class="STYLE2">
        <a href="https://arxiv.org/abs/2206.04658" target="_blank">BigVGAN: A Universal Neural Vocoder with Large-Scale Training</a>.&nbsp;[<a href="https://bigvgan-demo.github.io/" target="_blank">demo</a>]&nbsp;[<a href="https://github.com/NVIDIA/BigVGAN" target="_blank">code</a>]&nbsp;&nbsp;<em>ICLR</em> 2023.
        <br>Sang-gil Lee<strong><sup>+</sup></strong>, <strong>Wei Ping</strong>, Boris Ginsburg, Bryan Catanzaro, Sungroh Yoon.
        </p>

        <p class="STYLE2">
        <a href="https://arxiv.org/abs/2206.04624" target="_blank">Factuality Enhanced Language Models for Open-Ended Text Generation</a>.&nbsp;[<a href="https://github.com/nayeon7lee/FactualityPrompt" target="_blank">code</a>]&nbsp;&nbsp;<em>NeurIPS</em> 2022.
        <br>Nayeon Lee<strong><sup>+</sup></strong>, <strong>Wei Ping</strong>, Peng Xu, Mostofa Patwary, Pascale Fung, Mohammad Shoeybi, Bryan Catanzaro.
        </p>

        <p class="STYLE2">
        <a href="https://arxiv.org/abs/2202.04173" target="_blank">Exploring the Limits of Domain-Adaptive Training for Detoxifying Large-Scale Language Models</a>.&nbsp;[<a href="https://github.com/NVIDIA/Megatron-LM/tree/main/examples/detxoify_lm" target="_blank">code</a>]&nbsp;&nbsp;<em>NeurIPS</em> 2022.
        <br>Boxin Wang<strong><sup>+</sup></strong>, <strong>Wei Ping</strong>, Chaowei Xiao, Peng Xu, Mostofa Patwary, Mohammad Shoeybi, Bo Li, Anima Anandkumar, Bryan Catanzaro.
        </p>

<!--         <p class="STYLE2">
        <a href="https://arxiv.org/pdf/2107.02192.pdf" target="_blank">Long-Short Transformer: Efficient transformers for language and vision</a>.&nbsp;[<a href="https://github.com/NVIDIA/transformer-ls" target="_blank">code</a>]&nbsp;&nbsp;<em>NeurIPS</em> 2021.
        <br>Chen Zhu<strong><sup>+</sup></strong>, <strong>Wei Ping</strong>, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein, Anima Anandkumar, Bryan Catanzaro.
        </p> -->

        <p class="STYLE2">
        <a href="https://arxiv.org/pdf/2009.09761.pdf" target="_blank">DiffWave: A versatile diffusion model for audio synthesis</a>.&nbsp;[<a href="https://diffwave-demo.github.io/" target="_blank">project</a>]&nbsp;&nbsp;<em>ICLR</em> 2021 (Oral).
        <br>Zhifeng Kong<strong><sup>+</sup></strong>, <strong>Wei Ping</strong>, Jiaji Huang, Kexin Zhao, Bryan Catanzaro.
        </p>
    
        <p class="STYLE2">
        <a href="https://arxiv.org/pdf/1912.01219.pdf" target="_blank">WaveFlow: A compact flow-based model for raw audio</a>.&nbsp;[<a href="https://waveflow-demo.github.io/" target="_blank">project</a>]&nbsp;&nbsp;<em>ICML</em> 2020.
        <br><strong>Wei Ping</strong>, Kainan Peng, Kexin Zhao, Zhao Song.
        </p>

        <p class="STYLE2">
        <a href="https://arxiv.org/pdf/1905.08459.pdf" target="_blank">Non-autoregressive neural text-to-speech</a>.&nbsp;[<a href="https://parallel-neural-tts-demo.github.io/" target="_blank">demo</a>]&nbsp;&nbsp;<em>ICML</em> 2020.
        <br>Kainan Peng, <strong>Wei Ping</strong>, Zhao Song, Kexin Zhao.
        </p>

        <p class="STYLE2">
        <a href="https://arxiv.org/pdf/1807.07281.pdf" target="_blank">ClariNet: Parallel wave generation in end-to-end text-to-speech</a>.&nbsp;[<a href="https://clarinet-demo.github.io/" target="_blank">demo</a>]&nbsp;&nbsp;<em>ICLR</em> 2019.
        <br><strong>Wei Ping</strong>, Kainan Peng, Jitong Chen.
        </p>

        <p class="STYLE2">
        <a href="https://arxiv.org/pdf/1710.07654.pdf" target="_blank">Deep Voice 3: Scaling text-to-speech with convolutional sequence learning</a>.&nbsp;</a>[<a href="http://research.baidu.com/Blog/index-view?id=91" target="_blank">demo</a>][<a href="https://multi-speaker-clarinet-demo.github.io/" target="_blank">more samples</a>]&nbsp;&nbsp;<em>ICLR</em> 2018.
        <br><strong>Wei Ping</strong>, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang, Jonathan Raiman, John Miller.
        </p>

        <p class="STYLE2">
        <a href="https://arxiv.org/pdf/1802.06006" target="_blank">Neural voice cloning with a few samples</a>.&nbsp;<a href="https://audiodemos.github.io/" target="_blank">[demo]</a>&nbsp;&nbsp;<em>NeurIPS</em> 2018. 
        <br>Sercan O. Arik*, Jitong Chen*, Kainan Peng*, <strong>Wei Ping*</strong>, Yanqi Zhou.
        </p>

        <p class="STYLE2">
        <a href="https://arxiv.org/pdf/1705.08947.pdf" target="_blank">Deep Voice 2: Multi-speaker neural text-to-speech</a>.&nbsp;<a href="http://research.baidu.com/Blog/index-view?id=97" target="_blank">[demo]</a>&nbsp;&nbsp;<em>NeurIPS</em> 2017.
        <br>(&alpha;-&beta;) Sercan O. Arik, Gregory Diamos, Andrew Gibiansky, John Miller, Kainan Peng, <strong>Wei Ping</strong>, Jonathan Raiman, Yanqi Zhou.
        </p>

<!--         <p class="STYLE2"> 
        <a href="http://papers.nips.cc/paper/6342-learning-infinite-rbms-with-frank-wolfe.pdf" target="_blank">Learning infinite RBMs with Frank-Wolfe</a>.&nbsp;<em>NeurIPS</em> 2016.
        <br><strong>Wei Ping</strong>, Qiang Liu, Alexander Ihler.
        </p> -->

        <p class="STYLE2"> 
        <a href="https://arxiv.org/pdf/1511.02619.pdf" target="_blank">Decomposition bounds for marginal MAP</a>.&nbsp;&nbsp;</a><em>NeurIPS</em> 2015.
        <br><strong>Wei Ping</strong>, Qiang Liu, Alexander Ihler.
        </p>

<!--         <p class="STYLE2"> 
        <a href="http://proceedings.mlr.press/v32/ping14.pdf" target="_blank">Marginal structured SVM with hidden variables</a>.&nbsp;&nbsp;</a><em>ICML</em> 2014.
        <br><strong>Wei Ping</strong>, Qiang Liu, Alexander Ihler.
        </p> -->
  </font>
  </tr>
</table>
<hr />


</body>
</html>

